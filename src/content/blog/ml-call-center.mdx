---
title: "ML-Powered Monitoring Controls in a Financial Services Call Center"
date: "2025-07-05"
description: "How we built a transcript processing platform that powers traditional ML controls for regulatory compliance — starting with detecting non-self-identified complaints — and extends into LLM-driven summarization, coaching, and knowledge gap analysis."
tags: ["Machine Learning", "NLP", "Compliance", "GenAI", "LLM", "Call Center", "HITL", "RAG"]
contentType: "strategy"
traits: ["framework", "architecture", "case-study"]
---

{/* Hero section */}

### "How Do You Know You're Catching Every Complaint?"

That was the question. Not from a business leader. Not from an internal auditor. From a **regulator**.

In financial services, customer complaints are a regulatory obligation. The CFPB, OCC, and state regulators expect institutions to capture, track, and resolve every complaint. Missing one isn't a customer service failure — it's a compliance failure that can lead to consent orders and fines.

The existing process relied on agents recognizing complaints and manually logging them. This works when a customer says "I want to file a formal complaint." But complaints aren't always that explicit. A customer frustrated about a payment issue might vent, have it resolved by a skilled agent, and hang up satisfied — and nobody ever logs it. The agent handled the situation well; they just didn't recognize it as something that needed to be captured. These **non-self-identified complaints** — the ones that slip through because the agent's perception of "complaint" doesn't match the regulatory definition — are where risk accumulates.

When the regulator asks "how do you know?", "our agents are well-trained" isn't a sufficient answer. You need a **detective control** — a systematic, auditable mechanism that retrospectively identifies calls that should have been flagged but weren't.

That question was the catalyst. But what we ended up building was bigger than a single model. To detect missed complaints, we had to build a pipeline that processes every call transcript daily — tokenizing PII, normalizing text, and making it available for analysis. Once that infrastructure existed, it became a **monitoring platform**. Complaint capture was the first control we built on it. But the same transcript pipeline can power other regulatory controls (sales practice monitoring, UDAAP risk detection) and operational intelligence (coaching, summarization, knowledge gap analysis). The first category calls for traditional ML — explainable, deterministic, auditable. The second calls for generative AI.

This article covers how the platform works, walks through the **complaint capture model** as a worked example of traditional ML controls, and then explores how we're extending the same infrastructure with LLMs — what we've piloted, what we're designing, and how we think about which AI approach fits which problem.

### The First Decision: Traditional ML or GenAI?

Before any implementation, the most important architectural decision is which AI approach to apply to which problem. We found that it comes down to one question: **does the output need to be explained to a regulator?**

![Side-by-side comparison of Traditional ML and Generative AI approaches across explainability, determinism, cost, and comprehension dimensions](/images/blog/ml-call-center/ml-vs-genai.svg)

**Explainability.** When a regulator asks "why was this call flagged?", a Random Forest can point to specific feature importances — the words "escalate," "frustrated," and "supervisor" drove the score. The reasoning is transparent and auditable. An LLM can provide a natural-language explanation, but that explanation is itself generated — a post-hoc rationalization, not a trace of the actual computation. In a regulatory examination, that distinction matters.

**Determinism.** Same transcript into a traditional classifier twice produces the same score. Same transcript into an LLM twice might produce different results. This stochasticity is fine for internal analytics but problematic for compliance, where audit trails demand reproducibility.

**Output calibration.** Traditional ML produces numeric scores that can be thresholded, ranked, and monitored with standard statistical methods — drift detection and alerting are well-understood problems. LLM outputs are harder to monitor systematically.

**But traditional ML can't *understand* a call.** It detects. It scores. It cannot summarize a 45-minute conversation, identify that an agent gave incorrect policy information, or detect that a customer's frustration stemmed from a systemic processing issue. That deep language comprehension is what LLMs provide — and it's especially valuable when transcript quality varies, which it always does in speech-to-text.

The pragmatic answer is a **dual-track architecture**: traditional ML for regulatory-facing controls (anything that needs to be explained, reproduced, and audited) and GenAI for internal-facing intelligence (anything that needs comprehension and flexibility). Both tracks operate on the same data pipeline. The difference is the inference layer and the accountability standard applied to the output.

### The Shared Foundation: PII-Safe Transcripts

Regardless of which track a transcript feeds, it starts in the same place: a call recording platform that converts speech to text. We used Verint, but the pattern applies whether you're on NICE, AWS Transcribe, or any other platform.

Before that transcript touches any model, it goes through **PII tokenization**. Call transcripts in financial services routinely contain Social Security numbers, dates of birth, and card numbers — spoken aloud and faithfully captured. Our tokenization layer replaces these with opaque tokens using regex matching and named entity recognition: `[TOKEN_SSN]`, `[TOKEN_DOB]`, `[TOKEN_CARD]`.

The design principle: **neither model track needs PII to do its job.** Regulatory controls depend on sentiment-bearing words and behavioral patterns, not customer identifiers. Summarization needs to understand the *issue*, not the customer's Social Security number. Tokenizing upstream of everything simplifies the entire downstream architecture — fewer access controls, simpler retention policies, reduced blast radius.

This tokenization is one-way at the model layer. Even if a model were compromised, no sensitive data could be extracted.

![Dual-track pipeline showing Traditional ML for compliance and GenAI for internal intelligence branching from the same clean transcript source](/images/blog/ml-call-center/dual-track-pipeline.svg)

### Track 1: Traditional ML Controls (Regulatory-Facing)

The traditional ML track is where you build controls that need to produce explainable, deterministic, auditable outputs — the kind you can defend in a regulatory examination. Complaint capture was the first model we built on this track, and it's a good illustration of the pattern: NLP preprocessing, domain-specific feature engineering, a stable classifier, and human-in-the-loop validation. The same pattern applies to other regulatory controls — sales practice monitoring, UDAAP risk detection, fair lending analysis on call transcripts — with different labels, features, and review workflows but the same underlying architecture.

Here's how we built the complaint capture model — designed specifically to surface **non-self-identified complaints**, the calls that should have been logged but weren't because the agent didn't recognize them as complaints.

The model is a classic NLP pipeline built on Random Forest, with domain-specific feature engineering that made more difference than any algorithm choice.

**NLP Preprocessing.** Raw transcripts are noisy — conversational, full of filler, highly variable. The preprocessing pipeline applies four transformations.

First, **stopword removal** using a custom dictionary that goes well beyond the standard list. We added hundreds of call-center-specific filler terms ("um," "alright," "basically") that dilute the feature space without carrying complaint signal.

Second, **lemmatization** — reducing inflected words to base forms so that "payments," "payment," and "paying" map to a single feature.

Third, and most impactful, **synonym normalization**. We built manual dictionaries that collapsed complaint-signal words into canonical forms. "Disappointed," "frustrating," "dissatisfied," and "upset" all mapped to the same root. "Talk to manager," "speak to supervisor," and "escalate" converged to a single feature. Customers express dissatisfaction in dozens of ways — without this normalization, the model treats each variant as independent signal rather than reinforcing evidence of the same underlying complaint.

Finally, **Boolean Adjusted TF-IDF vectorization**. In long transcripts, word repetition reflects the nature of conversation, not complaint intensity. A customer says "payment" thirty times because the call is about a payment, not because the complaint is thirty times more severe. Boolean TF-IDF treats term presence as binary, then applies inverse document frequency — reducing noise dramatically for transcript-length documents.

![NLP preprocessing pipeline from raw transcript through stopword removal, lemmatization, synonym normalization, and TF-IDF vectorization](/images/blog/ml-call-center/nlp-preprocessing.svg)

**Why Random Forest.** We benchmarked over a dozen algorithms. Some linear models edged out Random Forest on raw AUC, but Random Forest offered the best combination of score separation (the ability to rank genuine missed complaints to the top of the distribution) and stability across time periods. In a compliance context, a model that gives consistent scores week over week is more valuable than one that fluctuates with retraining. We built this as a stable model that gets refitted only when drift monitoring indicates degradation — not on a continuous schedule.

**Handling Class Imbalance.** Non-self-identified complaints represent a tiny fraction of total call volume — under 2%. We addressed this through stratified resampling across months and call characteristics, and through a deliberate labeling choice: *feedback* cases — where a customer expressed concern that didn't meet the regulatory definition of a complaint — were labeled as non-complaint. This ensured the model learned the specific distinction regulators care about, not just "was the customer unhappy?"

**Deployment.** The model runs as a daily batch pipeline — pull transcripts, preprocess, score, rank — with a multi-day lag to account for transcription processing time. The scored output (a ranked list of calls most likely to contain unlogged complaints) is automatically routed to a quality management system for human review. Batch is appropriate here because detective controls are retrospective by definition — there's no value in real-time scoring for a call that already happened.

### Monitoring: The Human Loop That Closes the System

A model score is a recommendation. The human review process turns it into a compliance outcome.

Reviewers listen to flagged calls and assign verified labels: *complaint* (the model surfaced a genuine miss — this should have been logged), *feedback* (flagged but doesn't meet the regulatory definition), or *false positive*. Each reviewed call gets documented agent notes — creating the audit trail regulators expect.

![Human-in-the-loop QA review cycle showing model scoring, agent review, outcome classification, and feedback loop for retraining](/images/blog/ml-call-center/hitl-review-loop.svg)

This loop serves three purposes. First, **compliance remediation** — genuine missed complaints get logged and addressed. Second, **model improvement** — false positives and misses feed back as labeled data for retraining decisions. Third, **operational visibility** — a tracking dashboard gives leadership daily metrics on flagged calls, review rates, and true positive rates, providing the kind of systematic evidence a regulator expects.

One of the biggest learnings: the human review wasn't a bottleneck to be optimized away. It was one of the most valuable parts of the system. Agent notes provided qualitative context no model could generate. The QA dashboard gave leadership visibility they'd never had. The model narrows the haystack; the human makes the call.

### Track 2: Extending with LLMs

With a production pipeline processing every call transcript daily, we had the infrastructure to explore what generative AI could add. The key word here is *infrastructure* — the transcript pipeline, PII tokenization, and batch orchestration we built for regulatory controls became the foundation for a different set of capabilities.

**Call Summarization (Piloted).** This was our first LLM use case, and we chose it deliberately — it's high-value, low-risk, and the output is internal.

Reviewers in the complaint detection workflow were spending significant time listening to full call recordings to understand context before making a labeling decision. We piloted LLM-generated summaries — 3-5 sentence distillations of each transcript covering the customer's issue, the resolution provided, and the overall sentiment — and embedded them directly into the review workflow.

The pilot validated the concept: reviewers could triage flagged calls faster by scanning summaries first, reserving full-listen time for borderline cases. But moving from pilot to production surfaced the real engineering questions. How do you handle transcripts that exceed the model's context window? (Chunking with overlap, or pre-summarization of long calls.) How do you monitor summary quality at scale when there's no ground truth label? (Reviewer feedback signals and periodic sampling.) How do you manage prompt versioning so a prompt change doesn't silently alter summary behavior across the pipeline?

These are the problems we're solving now as we productionalize the summarization pipeline.

**Enterprise LLM Hosting.** One question we hear often: how do you actually call an LLM from inside a regulated enterprise environment? You're not making API calls to a public endpoint.

The answer, in our case, is **AWS Bedrock** — Amazon's managed service that provides access to foundation models (including Claude) within your own AWS environment. The critical properties for financial services:

**Data sovereignty.** Transcripts never leave your VPC. Bedrock invocations go through a VPC endpoint via AWS PrivateLink — traffic stays on the AWS backbone, never traversing the public internet.

**Zero data retention.** The model provider doesn't store your inputs or outputs. There's no training on your data. This isn't a trust-us assurance — it's a contractual and architectural guarantee.

**Model pinning.** You can lock to a specific model version, which matters for output consistency. When you change model versions, you do it deliberately and can test the impact before deployment.

Google Cloud's Vertex AI offers a similar pattern for organizations on GCP. The point isn't the specific cloud provider — it's that the deployment pattern exists for running LLM inference in a way that meets financial services data governance requirements without self-hosting.

![Enterprise LLM deployment architecture showing VPC-isolated inference through AWS Bedrock with PrivateLink, no data retention, and RAG integration](/images/blog/ml-call-center/enterprise-llm-deployment.svg)

**Coaching, Knowledge Gaps, and RAG (Designing Now).** The most ambitious extension — and the one we're still designing — is using LLMs to generate agent coaching insights grounded in actual policy documents.

The concept: an LLM evaluates a transcript against a coaching rubric (empathy, de-escalation, policy accuracy, resolution quality) and cross-references the agent's responses against a **RAG layer** — internal policies, SOPs, job aids, and training materials that have been chunked, embedded, and indexed in a vector store.

When there's a discrepancy — the agent quoted a 15-day grace period when the policy says 10 — the system flags it as a knowledge gap and recommends specific training. Aggregated across an agent's calls over weeks, this produces a coaching profile that's grounded in what actually happened, not abstract performance scores.

RAG is essential here because policies change constantly in financial services. A base LLM might confidently reference procedures that have since been updated. Retrieval-augmented generation ensures the model always grounds its answers in the latest version of the truth — the current documents, not its training data. For an industry where giving a customer the wrong information can itself become a compliance issue, this grounding is non-negotiable.

We're also exploring LLM-assisted **trend and theme analysis** — processing batches of transcripts to identify emerging complaint patterns that might signal systemic issues. A spike in "late fee" mentions across call centers could indicate a processing delay. A cluster of "app login" complaints might correlate with a recent software release. LLMs don't replace traditional analytics for trending — but they improve the front-end extraction step by reading how customers *actually describe* issues, including variations that don't map cleanly to predefined categories.

### Where the Tracks Reinforce Each Other

The dual-track architecture isn't just separation of concerns — the tracks actively improve each other.

LLM summaries **speed up the HITL review process** for the ML track. ML complaint scores **prioritize the LLM coaching pipeline** — rather than analyzing every call at LLM token rates, you focus on calls the model flagged. And RAG-powered knowledge gap detection **informs how analysts interpret complaint predictions** — "this agent's calls are being flagged because they're giving incorrect hardship program information, not because they're being rude."

The compound effect of both tracks on shared infrastructure is greater than either alone.

### Where This Is Heading

**Predictive detection.** Today's complaint model identifies missed complaints after they happen. The next iteration might identify calls *likely to result in complaints* based on early conversation signals — enabling intervention before a customer reaches the point of filing. The same principle applies to other controls: detecting emerging risk patterns before they become regulatory findings.

**Multi-modal analysis.** Speech-to-text loses information — tone, pace, pauses. Future systems will analyze audio directly, combining acoustic sentiment with text classification.

**Continuous coaching.** Rather than quarterly training refreshers, every agent receives a personalized weekly brief generated from their actual calls, referencing the policies they got wrong, with specific improvement recommendations.

The common thread: **call data is an underutilized source of enterprise intelligence.** Every conversation is a signal — about customer experience, product issues, agent performance, emerging risks. The question isn't whether to extract that intelligence. It's whether you're doing it in a way that's defensible for regulatory use cases and impactful for operational ones. Traditional ML where you need to explain. GenAI where you need to understand. Both on the same foundation.
